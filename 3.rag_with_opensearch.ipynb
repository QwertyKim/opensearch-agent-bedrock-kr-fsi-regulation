{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_retrieval_questions = [\n",
    "    \"How many total sessions are there at re:Invent?\", \n",
    "    \"List all sessions for developers.\", \n",
    "    \"How many GenAI-related sessions are there at this re:Invent?\", \n",
    "    \"What are the session codes related to API Gateway?\", \n",
    "    \"List all sessions targeted at data scientists.\", \n",
    "    \"How many sessions focus on AWS Lambda?\", \n",
    "    \"What is the number of sessions about cloud security?\", \n",
    "    \"List all sessions that mention Amazon S3.\" \n",
    "]\n",
    "\n",
    "contextual_exploration_questions = [ \n",
    "    \"Can you recommend sessions where I can learn about fine-tuning generative AI models?\", \n",
    "    \"Which sessions would be most beneficial for someone new to cloud computing?\", \n",
    "    \"What are the most relevant sessions about tools for Kubernetes?\", \n",
    "    \"Are there any sessions that explain the process of database migration?\", \n",
    "    \"Can you suggest sessions that focus on improving application performance?\", \n",
    "    \"What are the key sessions discussing serverless architecture?\", \n",
    "    \"Can you recommend sessions that cover best practices in DevOps?\", \n",
    "    \"What sessions might help me understand the integration of AI in cloud services?\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from string import Template\n",
    "\n",
    "def load_prompt(func_name, prompt_file='prompts.yaml'):\n",
    "    with open(prompt_file, 'r') as file:\n",
    "        prompts = yaml.safe_load(file)\n",
    "    \n",
    "    if func_name not in prompts:\n",
    "        raise KeyError(f\"Function '{func_name}' not found in the prompt file.\")\n",
    "    \n",
    "    func_prompts = prompts[func_name]\n",
    "        \n",
    "    if 'sys_template' in func_prompts and 'user_template' in func_prompts:\n",
    "        return func_prompts['sys_template'], func_prompts['user_template']\n",
    "    elif 'system_prompt' in func_prompts and 'user_prompt' in func_prompts:\n",
    "        return func_prompts['system_prompt'], func_prompts['user_prompt']\n",
    "    else:\n",
    "        raise KeyError(f\"'sys_template' or 'system_prompt' not found for '{func_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region = 'us-west-2'\n",
    "def init_bedrock_client(region):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "def converse_with_bedrock(model_id, sys_prompt, usr_prompt):\n",
    "    temperature = 0.5\n",
    "    top_p = 0.9\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def create_prompt(sys_template, user_template, **kwargs):\n",
    "    sys_prompt = [{\"text\": sys_template.format(**kwargs)}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template.format(**kwargs)}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "def create_prompt_without_parameter(sys_template, user_template):\n",
    "    sys_prompt = [{\"text\": sys_template}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "boto3_client = init_bedrock_client(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['AI/ML', 'Analytics', 'Architecture', 'Cloud Operations', 'Compute', 'Serverless & Containers', 'Database', 'Developer Tools', 'Security', 'Storage', 'Migration & Modernization', 'IoT', 'Other']\n",
    "audience_types = ['Developers', 'System Administrator', 'IT Administrator', 'Data Scientists', 'Security Professionals', 'Other']\n",
    "session_format = ['Breakout Session', 'Chalk Talk', 'Builder session', 'Workshop', 'Lightening Talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tool_selection(question):\n",
    "    sys_template, user_template = load_prompt('search_tool_selection')\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    sys_prompt, usr_prompt = create_prompt(sys_template, user_template, question=question)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, usr_prompt)\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in attribute_retrieval_questions:\n",
    "    response = search_tool_selection(question)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in contextual_exploration_questions:\n",
    "    response = search_tool_selection(question)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "host = os.getenv('OPENSEARCH_HOST')\n",
    "user = os.getenv('OPENSEARCH_USER')\n",
    "password = os.getenv('OPENSEARCH_PASSWORD')\n",
    "region = 'us-east-1'\n",
    "index_name = 'reinvent_session'\n",
    "\n",
    "os_client = OpenSearch(\n",
    "    hosts = [{'host': host.replace(\"https://\", \"\"), 'port': 443}],\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_text(topics, audience_types, session_format, question):\n",
    "    sys_template, user_template = load_prompt('search_by_text')\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    #model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    #model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt(sys_template, user_template, question=question, topics=topics, audience_types=audience_types, session_format=session_format)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "    return response['output']['message']['content'][0]['text'], sys_prompt, user_prompt\n",
    "\n",
    "question = \"How many total sessions are there at re:Invent?\"\n",
    "search_query, previous_sys_prompt, previous_user_prompt = search_by_text(topics, audience_types, session_format, question)\n",
    "print(\"search_query:\", search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(os_client, index_name, search_query):\n",
    "    try:\n",
    "        search_result = os_client.search(\n",
    "            index=index_name,\n",
    "            body=search_query\n",
    "        )\n",
    "        return search_result, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, failed_query, error_message):\n",
    "    sys_prompt, user_prompt = load_prompt('search_by_text_as_fallback')\n",
    "    \n",
    "    fallback_sys_prompt = sys_prompt.format(\n",
    "        previous_sys_prompt=previous_sys_prompt,\n",
    "        previous_user_prompt=previous_user_prompt,\n",
    "        failed_query=failed_query,\n",
    "        error_message=error_message\n",
    "    )\n",
    "\n",
    "    fallback_user_prompt = user_prompt\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    sys_prompt, usr_prompt = create_prompt_without_parameter(fallback_sys_prompt, fallback_user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, usr_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "### Test Case\n",
    "failed_query = \"\"\"{\n",
    "  \"size\": 0,\n",
    "  \"aggres\": {\n",
    "    \"total_sessions\": {\n",
    "      \"value_count\": {\n",
    "        \"field\": \"code\"\n",
    "      }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "search_result, error = execute_query(os_client, index_name, failed_query)\n",
    "print(error)\n",
    "revised_query = search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, failed_query, error)\n",
    "print(revised_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, failed_query, error_message):\n",
    "    fallback_sys_prompt = f\"\"\"\n",
    "    The previous attempt to create a query failed. Here are the details:\n",
    "\n",
    "    System Prompt used:\n",
    "    ```\n",
    "    {previous_sys_prompt}\n",
    "    ```\n",
    "\n",
    "    User Prompt used:\n",
    "    ```\n",
    "    {previous_user_prompt}\n",
    "    ```\n",
    "\n",
    "    The query generated from the above prompts:\n",
    "    {failed_query}\n",
    "\n",
    "    Error messages for failure:\n",
    "    {error_message}\n",
    "    \"\"\"\n",
    "\n",
    "    fallback_user_prompt = \"\"\"\n",
    "    Based on this information, please revise the query to fix this error and try again. \n",
    "    Provide only the revised DSL query without any additional explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt_without_parameter(fallback_sys_prompt, fallback_user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "    \n",
    "\n",
    "### Test Case\n",
    "failed_query = \"\"\"{\n",
    "  \"size\": 0,\n",
    "  \"aggres\": {\n",
    "    \"total_sessions\": {\n",
    "      \"value_count\": {\n",
    "        \"field\": \"code\"\n",
    "      }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "search_result, error = execute_query(os_client, index_name, failed_query)\n",
    "print(error)\n",
    "revised_query = search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, failed_query, error)\n",
    "print(revised_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_result, error = execute_query(os_client, index_name, search_query)\n",
    "\n",
    "    if error:\n",
    "        print(f\"attempt failed. Error: {error}\")\n",
    "        print(f\"Attepting fallback\")\n",
    "        revised_query = search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, search_query, error)\n",
    "        search_result, error = execute_query(os_client, index_name, revised_query)\n",
    "\n",
    "        if error:\n",
    "            raise Exception(\"Both initial and fallback queries failed. Final error: {error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_answer_with_text_search(search_result, search_query, question):\n",
    "    sys_prompt, user_prompt = load_prompt('generate_answer_with_text_search')\n",
    "    \n",
    "    search_query_str = json.dumps(search_query, indent=2)\n",
    "    search_result_str = json.dumps(search_result, indent=2)\n",
    "\n",
    "    user_prompt = user_prompt.format(\n",
    "        question=question,\n",
    "        search_query=search_query_str,\n",
    "        search_result=search_result_str\n",
    "    )\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt_without_parameter(sys_prompt, user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "answer = generate_answer_with_text_search(search_result, search_query, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(search_result, max_results=20):\n",
    "    hits = search_result.get('hits', {}).get('hits', [])\n",
    "\n",
    "    if hits:\n",
    "        total_hits = search_result.get('hits', {}).get('total', {}).get('value', 0)\n",
    "        formatted_results = []\n",
    "\n",
    "        for hit in hits[:max_results]:\n",
    "            source = hit.get('_source', {})\n",
    "            code = source.get('code', 'N/A')\n",
    "            title = source.get('title', 'N/A')\n",
    "            formatted_results.append(f\"{code}: {title}\")\n",
    "\n",
    "        result_str = \"\\n\".join(formatted_results)\n",
    "\n",
    "        if total_hits > max_results:\n",
    "            result_str += f\"\\n\\n... (Showing {max_results} out of {total_hits} results)\"\n",
    "\n",
    "        return result_str\n",
    "    else:\n",
    "        return json.dumps(search_result, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in attribute_retrieval_questions:\n",
    "    response = search_tool_selection(question)\n",
    "    \n",
    "    print(f\"\\n\\nquestion: {question}\")\n",
    "    if response == \"search_by_text\":\n",
    "        search_query, previous_sys_prompt, previous_user_prompt = search_by_text(topics, audience_types, session_format, question) \n",
    "        search_result, error = execute_query(os_client, index_name, search_query)\n",
    "        if error:\n",
    "            print(f\"attempt failed. Error: {error}\")\n",
    "            print(f\"Attepting fallback\")\n",
    "            revised_query = search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, search_query, error)\n",
    "            search_result, error = execute_query(os_client, index_name, revised_query)\n",
    "\n",
    "            if error:\n",
    "                raise Exception(\"Both initial and fallback queries failed. Final error: {error}\")\n",
    "            \n",
    "        if not error:\n",
    "            search_result_str = format_search_results(search_result)\n",
    "            answer = generate_answer_with_text_search(search_result_str, search_query, question)\n",
    "            print(f\"context:\\n {search_result_str}\")\n",
    "            print(f\"answer: {answer}\")\n",
    "    else:\n",
    "        print(f\"tool not selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(os_client, index_name, field, vector, weight, max_results):\n",
    "    query = {\n",
    "        \"size\": max_results,\n",
    "        \"_source\": [\"code\", \"title\", \"synopsis\"],\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                field: {\n",
    "                    \"vector\": vector,\n",
    "                    \"k\": max_results\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = os_client.search(index=index_name, body=query)\n",
    "        return [(hit['_source'], hit['_score'] * weight) for hit in response['hits']['hits']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_weighted_results(title_results, synopsis_results, max_results):\n",
    "    combined_results = title_results + synopsis_results\n",
    "\n",
    "    unique_results = {}\n",
    "    for result, score in combined_results:\n",
    "        code = result['code']  \n",
    "        if code in unique_results:\n",
    "            unique_results[code] = (result, max(score, unique_results[code][1]))\n",
    "        else:\n",
    "            unique_results[code] = (result, score)\n",
    "\n",
    "    sorted_results = sorted(unique_results.values(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results[:max_results]\n",
    "\n",
    "\n",
    "def search_by_similarity(os_client, question, max_results=10):\n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "    question_response = boto3_client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps({\"inputText\": question})\n",
    "    )\n",
    "    question_embedding = json.loads(question_response['body'].read())['embedding']\n",
    "\n",
    "    title_results = vector_search(os_client, index_name, \"title_embedding\", question_embedding, 0.4, max_results)\n",
    "    synopsis_results = vector_search(os_client, index_name, \"synopsis_embedding\", question_embedding, 0.6, max_results)\n",
    "\n",
    "    weighted_results = get_weighted_results(title_results, synopsis_results, max_results)\n",
    "    return weighted_results\n",
    "    \n",
    "\n",
    "question = \"Can you recommend sessions where I can learn about fine-tuning generative AI models?\"\n",
    "max_results = 10\n",
    "\n",
    "search_result = search_by_similarity(os_client, question, max_results)\n",
    "print(json.dumps(search_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_answer_with_similarity_search(search_result, question):\n",
    "    sys_prompt, user_prompt = load_prompt('generate_answer_with_similarity_search')\n",
    "\n",
    "    search_result_str = json.dumps(search_result, indent=2)\n",
    "\n",
    "    user_prompt = user_prompt.format(\n",
    "        question=question,\n",
    "        search_result=search_result_str\n",
    "    )\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt_without_parameter(sys_prompt, user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "question = \"Can you recommend sessions where I can learn about fine-tuning generative AI models?\"\n",
    "answer = generate_answer_with_similarity_search(search_result, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_query_with_llm(question):\n",
    "    sys_prompt, user_prompt = load_prompt('augment_query_with_llm')\n",
    "    user_prompt = user_prompt.format(question=question)\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt_without_parameter(sys_prompt, user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "question = \"Can you recommend sessions that cover best practices in DevOps?\"\n",
    "augmented_question = augment_query_with_llm(question)\n",
    "print(augmented_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_similarity_search_as_fallback(search_result, question, augmented_question):\n",
    "    sys_prompt, user_prompt = load_prompt('generate_answer_with_similarity_search_as_fallback')\n",
    "\n",
    "    search_result_str = json.dumps(search_result, indent=2)\n",
    "    user_prompt = user_prompt.format(\n",
    "        question=question,\n",
    "        search_result=search_result_str\n",
    "    )\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    sys_prompt, user_prompt = create_prompt_without_parameter(sys_prompt, user_prompt)\n",
    "    response = converse_with_bedrock(model_id, sys_prompt, user_prompt)\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in contextual_exploration_questions:\n",
    "    response = search_tool_selection(question)\n",
    "    \n",
    "    print(f\"\\n\\nquestion: {question}\")\n",
    "    if response == \"search_by_text\":\n",
    "        print(\"\\nsearch_by_text\\n\")\n",
    "        search_query, previous_sys_prompt, previous_user_prompt = search_by_text(topics, audience_types, session_format, question)\n",
    "        #print(\"search_query:\", search_query)\n",
    "        search_result, error = execute_query(os_client, index_name, search_query)\n",
    "        if error:\n",
    "            print(f\"attempt failed. Error: {error}\")\n",
    "            print(f\"Attepting fallback\")\n",
    "            revised_query = search_by_text_as_fallback(previous_sys_prompt, previous_user_prompt, search_query, error)\n",
    "            search_result, error = execute_query(os_client, index_name, revised_query)\n",
    "\n",
    "            if error:\n",
    "                max_results = 10\n",
    "                search_results = search_by_similarity(os_client, question, max_results)\n",
    "                answer = generate_answer_with_similarity_search(search_result, question)\n",
    "\n",
    "        if not error:\n",
    "            search_result_str = format_search_results(search_result)\n",
    "            answer = generate_answer_with_text_search(search_result_str, search_query, question)\n",
    "            print(f\"context:\\n {search_result_str}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nsearch_by_similarity\\n\")\n",
    "        max_results = 10\n",
    "        search_result = search_by_similarity(os_client, question, max_results)\n",
    "        #print(json.dumps(search_result, indent=2))        \n",
    "        answer = generate_answer_with_similarity_search(search_result, question)\n",
    "        print(\"1st answer:\", answer)\n",
    "\n",
    "        if answer == \"None\":\n",
    "            print(\"\\nAugmenting the user query with LLM\\n\")\n",
    "            augmented_question = augment_query_with_llm(question)\n",
    "            print(f\"\\nAugmented question: {augmented_question}\")\n",
    "            search_result = search_by_similarity(os_client, augmented_question, max_results)\n",
    "            print(f\"\\nSearch results: {search_result}\")\n",
    "            answer = generate_answer_with_similarity_search_as_fallback(search_result, question, augmented_question)\n",
    "\n",
    "    print(\"final answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
